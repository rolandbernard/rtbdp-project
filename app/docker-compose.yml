services:

  # Kafka Containers (Broker + UI)
  broker:
    image: confluentinc/cp-kafka:7.9.0
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-broker"
    restart: always
    ports:
      - "${PORT_BROKER:-29092}:29092"
    profiles:
      - dbui
      - producer
      - processor
      - frontend
    volumes:
      - broker-data:/var/lib/kafka/data:z
    environment:
      CLUSTER_ID: "${CLUSTER_ID:-YzUyZWRlYzBhNDkwNDNmNG}"
      KAFKA_NODE_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@broker:9093"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENERS: "INTERNAL://broker:9092,EXTERNAL://broker:29092,CONTROLLER://broker:9093"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://broker:9092,EXTERNAL://localhost:${PORT_BROKER:-29092}"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1" # this and next 3 options set to 1 as we have only one broker
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: "60000" # check whether to delete log segments every minute (default 5 minutes)
      KAFKA_LOG4J_ROOT_LOGLEVEL: "WARN" # optional, with next option reduces verboseness
      KAFKA_LOG4J_LOGGERS: "\
        kafka=WARN,\
        kafka.controller=WARN,\
        kafka.log.LogCleaner=WARN,\
        state.change.logger=WARN,\
        kafka.producer.async.DefaultEventHandler=WARN"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "broker:9092"]
      interval: 5s
      timeout: 5s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-kafka-ui"
    ports:
      - "${PORT_KAFKA_UI:-28080}:8080"
    profiles:
      - dbui
    depends_on:
      broker:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: "rtgh" # name shown in UI (Kafka UI can display multiple clusters)
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "broker:9092"
      LOGGING_LEVEL_ROOT: "WARN" # optional, along with next option reduces verbosity
      LOGGING_LEVEL_COM_PROVECTUS: "WARN"

  # PostgreSQL Containers (TimescaleDB + pgadmin)
  postgres:
    image: timescale/timescaledb:2.23.1-pg17
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-postgres"
    restart: always
    ports:
      - "${PORT_POSTGRES:-25432}:5432"
    profiles:
      - dbui
      - processor
      - ghareplay
      - frontend
    volumes:
      - ./postgres:/docker-entrypoint-initdb.d:z
      - postgres-data:/var/lib/postgresql/data:z
    environment:
      POSTGRES_DB: "db"
      POSTGRES_USER: "${USERID:-user}"
      POSTGRES_PASSWORD: "${USERPWD:-user}"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${USERID:-user}", "-d", "db"]
      interval: 5s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4:9.3
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-pgadmin"
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - dbui
    ports:
      - "${PORT_PGADMIN:-20080}:80"
    volumes:
      - ./pgadmin4/servers.json:/pgadmin4/servers.json:z
      - pgadmin-data:/var/lib/pgadmin:z
    environment:
      PGADMIN_DEFAULT_EMAIL: ${USEREMAIL:-user@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${USERPWD:-user}
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_DISABLE_POSTFIX: "true"
      GUNICORN_ACCESS_LOGFILE: "/dev/null"

  # ZooKeeper for enabling Flink High Availability
  zookeeper:
    image: zookeeper:3.9.4-jre-17
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-zookeeper"
    restart: always
    profiles:
      - processor
      - ghareplay
    volumes:
      - ./flink/logback.xml:/conf/logback.xml:z
    environment:
      ZOO_4LW_COMMANDS_WHITELIST: "ruok"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc -w 2 localhost 2181 | grep imok"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Flink Containers (Job Manager + Task Manager)
  flink-jobmanager:
    build: flink
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-flink:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-flink-jobmanager"
    restart: always
    depends_on:
      zookeeper:
        condition: service_healthy
    profiles:
      - processor
      - ghareplay
    ports:
      - "${PORT_FLINK:-8081}:8081"
    volumes:
      - flink-checkpoints:/opt/flink/checkpoints:z
      - flink-savepoints:/opt/flink/savepoints:z
      - flink-ha-metadata:/opt/flink/ha-metadata:z
    command: jobmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.scheduler: adaptive
        parallelism.default: ${FLINK_DEFAULT_PARALLELISM:-1}
        # Fault Tolerance Config
        state.backend: rocksdb
        state.backend.incremental: true
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: file:///opt/flink/checkpoints
        execution.checkpointing.savepoint-dir: file:///opt/flink/savepoints
        # Restart Strategy for the Jobs
        restart-strategy.type: fixed-delay
        restart-strategy.fixed-delay.attempts: 10
        restart-strategy.fixed-delay.delay: 10s
        # High Availability Configuration
        high-availability.type: zookeeper
        high-availability.storageDir: file:///opt/flink/ha-metadata
        high-availability.zookeeper.quorum: zookeeper:2181
        zookeeper.sasl.disable: true
      ROOT_LOG_LEVEL: WARN
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8081/overview"]
      interval: 5s
      timeout: 5s
      retries: 5

  flink-taskmanager:
    build: flink
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-flink:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-flink-taskmanager"
    restart: always
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    profiles:
      - processor
      - ghareplay
    volumes:
      - flink-checkpoints:/opt/flink/checkpoints:z
      - flink-savepoints:/opt/flink/savepoints:z
      - flink-ha-metadata:/opt/flink/ha-metadata:z
      - flink-rocksdb-local:/opt/flink/rocksdb-data:z
    command: taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: ${FLINK_TASK_SLOTS:-4}
        parallelism.default: ${FLINK_DEFAULT_PARALLELISM:-1}
        # Fault Tolerance Config
        state.backend: rocksdb
        state.backend.incremental: true
        state.backend.rocksdb.localdir: /opt/flink/rocksdb-data
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: file:///opt/flink/checkpoints
        execution.checkpointing.savepoint-dir: file:///opt/flink/savepoints
        # High Availability Configuration
        high-availability.type: zookeeper
        high-availability.storageDir: file:///opt/flink/ha-metadata
        high-availability.zookeeper.quorum: zookeeper:2181
        zookeeper.sasl.disable: true
      ROOT_LOG_LEVEL: WARN

  # Project-developed Containers (API Dummy + Producer + Processor + Frontend)
  ghdummy:
    build: ghdummy
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-ghdummy:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-ghdummy"
    ports:
      - "${PORT_GHDUMMY:-8889}:${PORT_GHDUMMY:-8889}"
    profiles:
      - ghdummy
    command: [
      "--port", "${PORT_GHDUMMY:-8889}",
      "--log-level", "warn"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:${PORT_GHDUMMY:-8889}/ok"]
      interval: 5s
      timeout: 5s
      retries: 5

  producer:
    build: producer
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-producer:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-producer"
    restart: always
    depends_on:
      broker:
        condition: service_healthy
    profiles:
      - producer
    command: [
      "--bootstrap-server", "broker:9092",
      "--url", "${GITHUB_URL:-http://ghdummy:${PORT_GHDUMMY:-8889}}",
      "--gh-token", "${GITHUB_TOKEN}",
      "--poll-ms", "${GITHUB_POLL_MS:-2250}",
      "--poll-depth", "${GITHUB_POLL_DEPTH:-300}",
      "--log-level", "warn"
    ]

  processor:
    build: processor
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-processor:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-processor"
    restart: on-failure
    volumes:
      - flink-checkpoints:/opt/flink/checkpoints:z
      - flink-savepoints:/opt/flink/savepoints:z
      - flink-ha-metadata:/opt/flink/ha-metadata:z
    depends_on:
      flink-jobmanager:
        condition: service_healthy
      broker:
        condition: service_healthy
      postgres:
        condition: service_healthy
    profiles:
      - processor
    command: [
      "-c", "com.rolandb.Processor",
      "/app/processor.jar",
      "--bootstrap-servers", "broker:9092",
      "--db-url", "jdbc:postgresql://postgres:5432/db",
      "--db-username", "${USERID:-user}",
      "--db-password", "${USERPWD:-user}"
    ]

  ghareplay:
    build: processor
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-processor:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-ghareplay"
    restart: on-failure
    volumes:
      - flink-checkpoints:/opt/flink/checkpoints:z
      - flink-savepoints:/opt/flink/savepoints:z
      - flink-ha-metadata:/opt/flink/ha-metadata:z
    depends_on:
      flink-jobmanager:
        condition: service_healthy
      postgres:
        condition: service_healthy
    profiles:
      - ghareplay
    environment:
      JOB_NAME: "GitHub Historical Event Analysis"
    command: [
      "-c", "com.rolandb.ProcessorHistory",
      "/app/processor.jar",
      "--start-date", "${GHAREPLAY_START}",
      "--end-date", "${GHAREPLAY_END}",
      "--db-url", "jdbc:postgresql://postgres:5432/db",
      "--db-username", "${USERID:-user}",
      "--db-password", "${USERPWD:-user}"
    ]

  frontend:
    build: frontend
    image: "${COMPOSE_PROJECT_NAME:-rtgh}-frontend:latest"
    container_name: "${COMPOSE_PROJECT_NAME:-rtgh}-frontend"
    restart: always
    depends_on:
      broker:
        condition: service_healthy
      postgres:
        condition: service_healthy
    ports:
      - "${PORT_FRONTEND:-8888}:${PORT_FRONTEND:-8888}"
      - "${PORT_FRONTEND_WS:-8887}:${PORT_FRONTEND_WS:-8887}"
    profiles:
      - frontend
    command: [
      "--bootstrap-server", "broker:9092",
      "--port", "${PORT_FRONTEND:-8888}",
      "--ws-port", "${PORT_FRONTEND_WS:-8887}",
      "--db-url", "jdbc:postgresql://postgres:5432/db",
      "--db-username", "${USERID:-user}",
      "--db-password", "${USERPWD:-user}",
      "--secret", "${FRONTEND_PWD}",
      "--log-level", "warn"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:${PORT_FRONTEND:-8888}"]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  broker-data:
  postgres-data:
  pgadmin-data:
  flink-checkpoints:
  flink-savepoints:
  flink-rocksdb-local:
  flink-ha-metadata:

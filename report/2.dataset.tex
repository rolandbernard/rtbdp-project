
\section{Description of the Data Sources} \label{dataset}

\subsection{GitHub Events API}

The primary source of data for this project is the official \href{https://docs.github.com/en/rest/activity/events}{GitHub Events API} \cite{events}, specifically the \inlinecode{api.github.com/events} HTTP endpoint. This endpoint provides the most recent events from public activity across the entire GitHub platform. The data can be retrieved via periodic polling of the HTTP endpoint to effectively get a high-volume stream of events. Each event is delivered as a JSON object containing detailed information about the action performed.

However, this data source has some limitations relevant to the project. Firstly, the endpoint only offers the 300 most recent events. This means that it is not possible to get older events. Additionally, a maximum of 100 events can be retrieved with a single request, necessitating a mechanism to request three pages at once to capture the full 300-event buffer. Further, the events are not updated continuously, instead testing suggests that all 300 events seem to be replaced every second. Finally, the API has strict rate limits associated with unauthenticated requests, requiring the system to use personal access tokens.

\subsection{Dummy API and GHArchive}

To facilitate offline development and testing without exhausting API rate limits, and without needing any real access token at all, a dummy API server was developed. This component serves historical data sourced from \href{https://www.gharchive.org/}{GHArchive} \cite{gharchive}. Note that while GHArchive does not require any access token, it is not suitable as the main data source because it is two to three hours delayed and does not include all events. For testing, the dummy API server allows for a configurable initial delay and speed-up factors, enabling the simulation of either high-velocity traffic patterns for stress-testing the Flink pipeline, or low-velocity traffic for easier debugging of potential issues, all without relying on the live GitHub API.

\subsection{Data Characteristics}

\begin{figure}[ht]
    \centering
\begin{verbatim}{
  "id": "8251545586",
  "type": "PushEvent",
  "actor": {
    "id": 121951544,
    "login": "movieflixgr",
    "display_login": "movieflixgr",
    "gravatar_id": "",
    "url": "https://api.github.com/users/[..]",
    "avatar_url": "https://avatars.github[..]"
  },
  "repo": {
    "id": 1126450259,
    "name": "movieflixgr/Subtitles",
    "url": "https://api.github.com/repos/[..]"
  },
  "payload": {
    "repository_id": 1126450259,
    "push_id": 30546526697,
    "ref": "refs/heads/main",
    "head": "d5e986993aa47729cb18a82ac9d1[..]",
    "before": "6bf91f2d0600084cc7218c1934[..]"
  },
  "public": true,
  "created_at": "2026-02-08T20:09:50Z"
}\end{verbatim}
    \caption{An example event extracted from the GitHub Events API. It shows the most common type of event, corresponding to the push of a commit to a repository. Some values have been truncated for brevity, indicated with \inlinecode{[..]}.}
    \label{fig:event}
\end{figure}

\paragraph{Variety}
The data is highly heterogeneous. The API returns JSON objects representing a wide array of event types, including events for commit pushes, opening/closing/merging pull requests or issues, users starring or forking repositories, and comments created on issues, pull requests, or commits. \Cref{fig:event} shows an example of an event representing the push of commits to a repository. This is the most common, but also one of the simplest event types. Each event type possesses a unique nested JSON structure containing detailed information about the actor, the repository, and the specific action payload, which can be significantly more complex than the example shown in \Cref{fig:event}.

\paragraph{Volume and Velocity}
Even considering only public events, the volume of events on GitHub is still significant. Observations during the project indicate a rate of 70-120 events per second, depending on the time of day. This accumulates to millions of events per day, translating to roughly 500-600 MiB of data per hour in raw events. While the project implementation defaults to a polling interval of 2.25 seconds to respect rate limits, the architecture is capable of handling much higher frequencies; the live demo, for instance, operates with a polling interval of 750ms. Generally there is little benefit to reducing the polling interval below one second, since the events on the GitHub Events API are only updated once a second.

\paragraph{Veracity and Delay}
A critical characteristic of this data source is the inherent latency. Events retrieved from the standard \inlinecode{api.github.com/events} endpoint are typically about 5 minutes behind real-time. Furthermore, observation has shown that while roughly 95\% of events arrive in chronological order, out-of-order delivery is possible. Analysis suggests that 99.8\% of events are less than 10 seconds out of order, and 99.9\% are within a 5-minute window. However, extreme outliers exist where events may be delayed by several days. This project handles the latter case by dropping events that are too far delayed, motivated by the fact that they represent only a small fraction of all events.


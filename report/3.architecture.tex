
\section{\texorpdfstring{Technologies and \\Overall Architecture}{Technologies and Overall Architecture}} \label{architecture}

\subsection{Technology Stack}

\paragraph{Backend Components}
The backend services, i.e., the producer, the frontend server, and the Flink based processor, are implemented in Java \cite{java21}. Apache Maven \cite{mavenDocs} is used for dependency management and as a build system. Both the producer and the frontend server rely on RxJava \cite{rxJavaDocs} for reactive stream processing and managing asynchronous operations. In all backend components the Argparse4j library \cite{argparse4jDocs} is used to handle command-line configuration, while Jackson \cite{jacksonDocs} is utilized for high-performance JSON parsing. Logging is standardized by using SLF4J \cite{slf4jDocs} and Logback \cite{logbackDocs}, with JUnit \cite{junitDocs} used for unit testing to ensure code reliability.

\paragraph{Frontend Client}
The user interface of the project is implemented as a Single Page Application (SPA) built with TypeScript \cite{tsDocs} and React \cite{reactDocs}. Vite \cite{viteDocs} is used to serve as the build toolchain. React Router \cite{reactRouterDocs} is used to handle client-side routing. Styling is handled via Tailwind CSS \cite{tailwindDocs}, and data visualization is powered by the React based Recharts \cite{rechartsDocs} library. Communication with the backend utilizes RxJS \cite{rxjsDocs} to manage WebSocket streams and simple stream operations.

\paragraph{Stream Processing and Storage}
Further, the project makes use of a number of different technologies for the storage, processing, and streaming transmission of data. Firstly, Apache Kafka \cite{kafkaDocs,kreps2011kafka} is used as the central system for data ingestion and messaging in the project. Next, Kafka Flink \cite{flinkDocs,carbone2015apache} serves as the core stream processing engine in the project. Flink has been configured to utilize the RocksDB \cite{rocksdbDocs} state backend to manage large state sizes and enable fault tolerance. Flink is further configured for high availability, by using Apache ZooKeeper \cite{zookeeperDocs}. To provide persistent storage for processed data, the project uses PostgreSQL \cite{postgresDocs}, a popular open-source database management system. The project also uses the TimescaleDB extension \cite{timescaleDocs}, providing facilities for managing time-series data, especially partitioning and retention. Finally, the project makes use of Docker \cite{dockerDocs} and Docker Compose \cite{dockerCompose} to orchestrate all the services.

\subsection{System Architecture and Data Flow}

The project is built on a variation of the provided reference architecture. \Cref{fig:arch} shows an overview of the overall system architecture with all important components. The system follows a modular architecture, orchestrated via Docker Compose. The workflow is divided into four distinct stages, data ingestion, stream processing, data serving, and display of the results in the frontend.

\input{tikzfigures/architecture.tex}

\subsubsection{Data Ingestion}
A custom Kafka Producer is responsible for interfacing with the GitHub Events API. It polls the GitHub Events API at a configurable interval and depth. The code is implemented to respect GitHub's rate limiting, automatically pausing if the limit is hit. To capture all events, the polling retrieves overlapping windows of events with subsequent polls. Therefore, after receiving the list of events from the GitHub API, this component will also handle deduplication of events. For this task, the producer maintains a cache of the most recent event IDs. The producer then publishes the deduplicated raw GitHub events as JSON serialized strings to a dedicated Kafka topic, called \inlinecode{raw_events}.

During real deployment of the application, it is intended that the producer polls from the official GitHub API. However, for testing and development purposes a dummy API server has been developed. It offers the same \inlinecode{/events} endpoint, but does not require a real access token and has configurable throughput. The producer interacts with it in the same way as the official API.

\subsubsection{Stream Processing}
The core processing on the events is performed in a custom Apache Flink job. It is used to process the stream of events to extract relevant information and compute various aggregations to derive meaningful analytics. It consumes raw events from the \inlinecode{raw_events} Kafka topic, performs several complex processing operations. The transformations performed are organized into a series of modular pipelines, each responsible for feeding one table in PostgreSQL and one topic in Kafka. The following subsections discuss the main processing steps performed in the Flink based processor.

\paragraph{Preprocessing and Time Management} \label{preproc}
The first step in the processing pipeline is to parse the raw JSON events. The job operates in event time by assigning as timestamp the \inlinecode{created_at} timestamp extracted from the JSON payload. A watermark strategy tolerating up to 10 seconds of out-of-orderness is applied. Late events beyond this window are handled via specific allowed lateness strategies depending on the window type of different later operations.

Events are then passed on to subsequent steps, one of which parses the event payloads and generates human-readable descriptions, e.g., \inlinecode{"Roland pushed 3 commits to repo"}. It then takes this description, together with the event kind, the repository ID, and the user ID, as an output. These processed events are what is used in the frontend client to implement the live events stream.

\paragraph{User and Repository Information}
Not all events involving a specific user or repository contain all information about the concerning entry. For example, in some cases we only get information about the user ID, and are missing the rest of the user information. To improve the user interface, the processor therefore contains some transformations for extracting and aggregating user and repository details. For each event, we extract from the payload all the user and repository details.

Since events often contain only partial information about an entity, these extracted details have to be merged on a per-field basis, as compared to the full row, as is done with the other computation results generated by the Flink job. To handle this, these pipelines assign per-field sequence numbers to each update. The downstream database sink uses these sequence numbers to ensure that only that part of a row is updated for which incoming data is strictly newer than what is currently stored. A similar merging is applied on the frontend client using the real-time updates it receives.

\paragraph{Windowed Aggregation}
For historical data display, primarily for the charts in the frontend UI, the processor job performs windowed aggregation. It uses standard Flink Tumbling Windows of 10 seconds and 5 minutes duration to build long-term history of event counts. Event counts are computed for multiple different groupings. First, event count aggregations are computed on a per-type basis. Further, they are also computed per-user and per-repository. Finally, in addition to the total event count, for repositories the job also computes the count of new stars, which is used as an input to the trending score computation.

For real-time counters, standard sliding windows as provided by Flink proved too inefficient. Instead, the system uses a custom operator. This operator maintains circular buckets to aggregate counts efficiently across multiple overlapping window sizes of 5 minutes, 1 hour, 6 hours, and 24 hours simultaneously. This allows the dashboard to show the events in the last 24 hours updated every second without the overhead of maintaining millions of window objects.

\paragraph{Ranking and Trending}
To generate the user and repository rankings, the process consumes the output of the real-time counters. A custom operator is used to compute and maintain the complete ranking for each category. However, outputting the complete ranking, or even just the rows that have shifted, for each time a live counter changes, would be prohibitively expensive. Instead, the operator simply outputs an update including only the row for which the count changed. However, crucially, the updates produced by the operator include both the new and the old row number, allowing the frontend client to locally reconstruct exactly how the ranking changed.

For the global lists of trending repositories, the number of new stars is used. A custom operator calculates a composite score by combining new star counters across different time horizons, allowing the system to flag repositories experiencing a sudden surge in interest. The trending score is computed as
\begin{align*}
    t_\mathrm{score} = 10 s_{5\mathrm{m}} + 5 s_{5\mathrm{h}} + 2 s_{6\mathrm{h}} + s_{24\mathrm{h}} ,
\end{align*}
where $s_t$ is the number of new stars the repository has received in the latest time window of length $t$. This is a relatively simplistic model, but works reasonably well to identify trending repositories. By assigning a higher weight to more recently obtained stars, repositories that are experiencing immediate popularity are weighted more highly, but long term popularity is still taken into account.

\paragraph{Output of Results}
After performing the processing steps to compute the different analytics, the results are first written to PostgreSQL tables. Row updates are modeled as upserts, i.e., they represent an insert or an update of a single row. During this process each row is assigned a sequence number. This number is used by the client to know whether a row it receives from the frontend server is newer or older than the one it may already have. After writing the result to PostgreSQL, the row updates are sent to dedicated Kafka topics. Note that results are sent to Kafka only after they have been committed into PostgreSQL. 
This sequence ensures data consistency. By committing to PostgreSQL before Kafka, the system prevents a race condition where a frontend snapshot read from the database might miss updates already broadcast over the Kafka stream. After outputting the results to PostgreSQL and Kafka, they are ready to be consumed by the frontend.

\subsubsection{Data Serving}
A Java-based server acts as a bridge between the backend and the frontend client. It serves static assets and hosts a WebSocket endpoint. The processed data from Flink, that has been published to PostgreSQL tables and Kafka topics, is made available to the frontend client via a WebSocket API. It combines data queried from Postgres with real-time streams consumed from Kafka. The way this works is that the frontend client can request either reads from the table, or subscribe to a specific table with filters. In case of a table read, the frontend server connects to PostgreSQL and then forwards the results to the client. For real-time updates, the frontend server connects to all Kafka topics when starting the server, avoiding having to open a separate Kafka consumer for each client. Then, for each event it receives from Kafka, finds all clients that currently have a subscription matching the event, forwarding the event to those clients.

\subsubsection{Frontend}
The user interface is implemented as a custom single-page application using React and React Router, served by the frontend server. It offers an interactive dashboard with dynamic charts that update in real time as new data arrives. To obtain the real-time data, it connects via WebSocket to the frontend server. Upon connection, it subscribes to live updates, and then requests an initial snapshot from the backend, which is sourced from the DB. Doing it in this order ensures that no events are missed, and the sequence numbers are used to ensure new rows are not overwritten with old ones. As new messages arrive via WebSocket, the charts and counters increment in real time.

\subsubsection{Storage}

The database serves two purposes. Firstly, it is required for providing initial state for the dashboard, without having to wait for all values to update or replaying old events from Kafka. Second, it enables the retrieval of historical results. The database contains a table for each of the aspects computed by the Flink process, such as the events stream, the user and repository details, as well as the live and historical aggregations of event counts. TimescaleDB hypertables are used for some tables for automatic partitioning and using a retention policy to delete old data.

For the rankings, virtual views are used to compute row numbers and ranks based on the live scores. These are virtual views instead of proper tables because the high frequency of updates causes the rankings to change frequently. Since the change of a single row in the live scores could cause a cascade of row changes, using a materialized table would lead to unsustainable update volumes. These are two views to ensure low-latency queries for the frontend. One that is faster when retrieving a range of rows, and one that is faster for querying a very small number of rows. The frontend server dynamically switches between these for best performance.

\subsubsection{Messaging}

The final component of the architecture is formed by Apache Kafka. Kafka serves to decouple the ingestion from the processing, as well as the processing from the serving of the transformation results in the frontend. In addition to a topic used for the stream of raw events generated by the processor, the architecture utilizes specific topics for each of the outputs generated by the Flink processor. The Flink processor publishes processed results to these output topics for the frontend to consume in real time.

\subsection{Scalability and Reliability}

The system is designed for both resilience and scalability and efficiency.

\paragraph{Fault Tolerance}
Flink is configured to take incremental checkpoints of the RocksDB backend. In the event of a TaskManager failure, the job restarts from the last checkpoint, ensuring at-least-once processing semantics of the pipeline. At-least-once semantics are acceptable for this project, since all the outputs of the Flink processor are handled as upserts to the tables. Additionally, Flink has also been configured for high availability with the help of a ZooKeeper container. This means that the system is able to recover from JobManager failure, allowing jobs to restart from the last checkpoint using the metadata stored in ZooKeeper.

Further, if the processing layer goes offline for maintenance or unexpected failure, Kafka retains the message log. Upon restart, Flink processes the backlog to catch up. If however the producer goes down, events will be lost. This is unavoidable because the GitHub Events API does not offer any way to acquire older events than the last 300. One way to mitigate the issue would be by running multiple producers in parallel. This would cause duplicate events to be written to Kafka, but this would be acceptable since there is an additional deduplication step in the Flink processor.

\paragraph{Handling Out-of-Order Data}
As discussed in \Cref{preproc}, a watermark strategy tolerating up to 10 seconds of out-of-orderness is applied in the Flink processor. For the historical windowed aggregations, a specific lateness, 50 minutes for 5-minute windows and 100 seconds for the 10-second window, is additionally allowed for late-arriving events. This will cause an update to previously emitted window results, ensuring eventual consistency. A similar approach is taken for the live window aggregations. Here late events are used only to update the most recently emitted window, and any window emitted in the future.

For row updates in the frontend client, simply applying the latest arriving event is incorrect because the results of the snapshot replay may arrive after the real-time updates from the subscription. To solve this issue, the system assigns a sequence number at the ingestion point before the database sink. The client then only considers the latest version of each row by dropping events that arrive late with an earlier sequence number.

\paragraph{Scalability}
To improve scalability of the application, the Kafka topics are partitioned, allowing the Flink job to scale out its parallelism. Furthermore, most operators in the Flink processing job, such as parsing, windowing, mapping are fully parallel.

However, there are some limitations to the parallelism in the Flink processor. Global ranking requires gathering data to a single node to compare with all other values, which represents a theoretical bottleneck. Though sufficient for the current volume of GitHub data, this is an aspect of the architecture that is currently not scalable. Note that parallelization is still possible between the different rankings, i.e., ranking per-user and per-repository activity can be computed independently in parallel. Similarly, the rankings for different time window lengths can be computed in parallel.

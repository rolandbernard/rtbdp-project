
\section{\texorpdfstring{Technologies and \\Overall Architecture}{Technologies and Overall Architecture}} \label{architecture}

\subsection{Technology Stack}

\paragraph{Backend Components}
The backend services, i.e., the producer, the frontend server, and the Flink based processor, are implemented in Java \cite{java21}. Apache Maven \cite{mavenDocs} is used for dependency management and as a build system. Both the producer and the frontend server rely on RxJava \cite{rxJavaDocs} for reactive stream processing and managing asynchronous operations. In all backend components the Argparse4j library \cite{argparse4jDocs} is used to handle command-line configuration, while Jackson \cite{jacksonDocs} is utilized for high-performance JSON parsing. Logging is standardized in this project by using SLF4J \cite{slf4jDocs} and Logback \cite{logbackDocs}, with JUnit \cite{junitDocs} used for unit testing to ensure code reliability.

\paragraph{Frontend Client}
The user interface of the project is implemented as a Single Page Application (SPA) built with TypeScript \cite{tsDocs} and React \cite{reactDocs}. Vite \cite{viteDocs} is used to serve as the build toolchain. Styling is handled via Tailwind CSS \cite{tailwindDocs}, and data visualization is powered by the React based Recharts \cite{rechartsDocs} library. Communication with the backend utilizes RxJS \cite{rxjsDocs} to manage WebSocket streams and simple stream operations.

\paragraph{Stream Processing and Storage}
Further, the project makes use of a number of different technologies for the storage, processing, and streaming transmission of data. Firstly, Apache Kafka \cite{kafkaDocs,kreps2011kafka} is used to serve as the central system for data ingestion and messaging in the project. Next, Kafka Flink \cite{flinkDocs,carbone2015apache} serves as the core stream processing engine in the project. In this project it has further been configured to utilize the RocksDB \cite{rocksdbDocs} state backend to manage large state sizes and enable fault tolerance. Flink is further configured in this project for high availability, by using Apache ZooKeeper \cite{zookeeperDocs}. To provide persistent storage for processed data, the project uses PostgreSQL \cite{postgresDocs}, a popular open-source database management system. The project also uses the TimescaleDB extension \cite{timescaleDocs}, providing facilities for managing time-series data, especially partitioning and retention. Finally, the project makes use of Docker \cite{dockerDocs} and Docker Compose \cite{dockerCompose} to orchestrate all the services.

\subsection{System Architecture}

The project is build on a variation of the provided reference architecture. \Cref{fig:arch} shows and overview of the overall system architecture with all important components. The system follows a modular architecture, orchestrated via Docker Compose. The workflow is divided into four distinct stages, data ingestion, stream processing, data serving, and display of the results in the frontend.

\input{tikzfigures/architecture.tex}

\subsubsection{Data Ingestion}
A custom Java-based Kafka Producer is responsible for interfacing with the GitHub Events API. It polls the GitHub Events API at a configurable interval and depth. The code is implemented to respect GitHub's rate limiting, automatically pausing if the limit is hit. After receiving the list of events from the GitHub API, this component will further handle de-duplication of events, as the API may return overlapping events in subsequent polls. For this task, the producer maintains a cache of the most recent event IDs. The producer then publishes the raw GitHub events as JSON serialized strings to a dedicated Kafka topic, called \inlinecode{raw_events} in this project.

During real deployment of the application, it is intended that the producer polls from the official GitHub API. However, for testing and development proposes a dummy API server has been developed. It offers the same \inlinecode{/events} endpoint, but does not require a real access token and has configurable throughput. The producer interacts with it in the same way as the official API.

\subsubsection{Stream Processing}
The core processing on the events is performed in a custom Apache Flink job. It is used to process the stream of events to extract relevant information and compute various aggregations to derive meaningful analytics. It consumes raw events from the \inlinecode{raw_events} Kafka topic, performs several complex processing operations.




\subsubsection{Data Serving}
The processed data from Flink will be published to new Kafka topics. A Java-based server will consume these topics and expose the results via a WebSocket connection to the frontend.

\subsubsection{Frontend}
The user interface will be a custom single-page application using HTML and JavaScript, served by the frontend server. It will leverage the ECharts library to create an interactive dashboard with dynamic charts that update in real-time as new data arrives.

\subsubsection{Storage and Data Model}

\subsubsection{Messaging}


\subsection{Scalability and Reliability}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec nisi risus, suscipit sed mattis non, sagittis sed nisi. Sed a lobortis odio, quis dapibus nisl. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Fusce tincidunt sapien eu dolor ullamcorper, quis dapibus lectus venenatis. Nunc a blandit massa. Fusce ac fringilla nibh, nec eleifend mauris. Sed maximus tempus nibh, porta cursus mi rutrum et. Nunc erat massa, commodo id ornare ut, scelerisque ac est. Cras elit metus, aliquet sit amet maximus in, gravida in mauris. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Ut enim tellus, ornare quis malesuada non, scelerisque a elit. Donec dictum velit eu dui hendrerit ultrices. Pellentesque posuere porta odio eu feugiat. Phasellus non malesuada libero. Suspendisse sit amet tempus orci, quis dictum tellus. Proin magna massa. 

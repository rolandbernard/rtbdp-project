
\section{\texorpdfstring{Technologies and \\Overall Architecture}{Technologies and Overall Architecture}} \label{architecture}

\subsection{Technology Stack}

\paragraph{Backend Components}
The backend services, i.e., the producer, the frontend server, and the Flink based processor, are implemented in Java \cite{java21}. Apache Maven \cite{mavenDocs} is used for dependency management and as a build system. Both the producer and the frontend server rely on RxJava \cite{rxJavaDocs} for reactive stream processing and managing asynchronous operations. In all backend components the Argparse4j library \cite{argparse4jDocs} is used to handle command-line configuration, while Jackson \cite{jacksonDocs} is utilized for high-performance JSON parsing. Logging is standardized in this project by using SLF4J \cite{slf4jDocs} and Logback \cite{logbackDocs}, with JUnit \cite{junitDocs} used for unit testing to ensure code reliability.

\paragraph{Frontend Client}
The user interface of the project is implemented as a Single Page Application (SPA) built with TypeScript \cite{tsDocs} and React \cite{reactDocs}. Vite \cite{viteDocs} is used to serve as the build toolchain. Styling is handled via Tailwind CSS \cite{tailwindDocs}, and data visualization is powered by the React based Recharts \cite{rechartsDocs} library. Communication with the backend utilizes RxJS \cite{rxjsDocs} to manage WebSocket streams and simple stream operations.

\paragraph{Stream Processing and Storage}
Further, the project makes use of a number of different technologies for the storage, processing, and streaming transmission of data. Firstly, Apache Kafka \cite{kafkaDocs,kreps2011kafka} is used to serve as the central system for data ingestion and messaging in the project. Next, Kafka Flink \cite{flinkDocs,carbone2015apache} serves as the core stream processing engine in the project. In this project it has further been configured to utilize the RocksDB \cite{rocksdbDocs} state backend to manage large state sizes and enable fault tolerance. Flink is further configured in this project for high availability, by using Apache ZooKeeper \cite{zookeeperDocs}. To provide persistent storage for processed data, the project uses PostgreSQL \cite{postgresDocs}, a popular open-source database management system. The project also uses the TimescaleDB extension \cite{timescaleDocs}, providing facilities for managing time-series data, especially partitioning and retention. Finally, the project makes use of Docker \cite{dockerDocs} and Docker Compose \cite{dockerCompose} to orchestrate all the services.

\subsection{System Architecture and Data Flow}

The project is build on a variation of the provided reference architecture. \Cref{fig:arch} shows and overview of the overall system architecture with all important components. The system follows a modular architecture, orchestrated via Docker Compose. The workflow is divided into four distinct stages, data ingestion, stream processing, data serving, and display of the results in the frontend.

\input{tikzfigures/architecture.tex}

\subsubsection{Data Ingestion}
A custom Java-based Kafka Producer is responsible for interfacing with the GitHub Events API. It polls the GitHub Events API at a configurable interval and depth. The code is implemented to respect GitHub's rate limiting, automatically pausing if the limit is hit. To capture all events, the polling retrieves overlapping windows of events with subsequent polls. Therefore, after receiving the list of events from the GitHub API, this component will further handle deduplication of events. For this task, the producer maintains a cache of the most recent event IDs. The producer then publishes the deduplicated raw GitHub events as JSON serialized strings to a dedicated Kafka topic, called \inlinecode{raw_events} in this project.

During real deployment of the application, it is intended that the producer polls from the official GitHub API. However, for testing and development proposes a dummy API server has been developed. It offers the same \inlinecode{/events} endpoint, but does not require a real access token and has configurable throughput. The producer interacts with it in the same way as the official API.

\subsubsection{Stream Processing}
The core processing on the events is performed in a custom Apache Flink job. It is used to process the stream of events to extract relevant information and compute various aggregations to derive meaningful analytics. It consumes raw events from the \inlinecode{raw_events} Kafka topic, performs several complex processing operations. The transformations performed are organized into a series modular pipelines, each responsible for feeding one pair of a table in PostgreSQL and a topic in Kafka. The following are the main processing steps performed in the Fink based processor.

\paragraph{Preprocessing and Time Management}
The first step in the processing pipeline is to parse the raw JSON events. The job operates in Event Time by assigning as timestamp the \inlinecode{created_at} timestamp extracted from the JSON payload. A watermark strategy tolerating up to 10 seconds of out-of-orderness is applied. Late events beyond this window are handled via specific allowed lateness strategies depending on the window type of different later operations. These are then passed on to further processing steps.

One of these processing steps parses the event payloads and generates human-readable descriptions, e.g., \inlinecode{"Roland pushed 3 commits to repo"}. It then takes this description, together with the event kind, the repository ID, and the user ID, as an output. These processed events are what is used in the frontend client to implement the live events stream.

\paragraph{User and Repository Information}
Not all events involving a specific user or repository contain all information about the concerning entry. For example, in some cases we only get information the user ID, and are missing the rest of the user information. In order to enable a better user interface the processor therefore contains some transformations for extracting and aggregating user and repository details. For each event, we extract from the payload all the user and repository details.

Since events often contain only partial information about an entity, these extracted details have to be merged on a per-field basis, as compared to the full row, as is one with the other computation results generated in the Flink job. To handle this, these pipelines assign per-field sequence numbers to each update. The downstream database sink uses these sequence numbers to ensure that only that part of a row is updated for which incoming data is strictly newer than what is currently stored. A similar merging is applied on the frontend client using the real-time updates it receives.

\paragraph{Windowed Aggregation}
For historical data display, primarily for the charts in the frontend UI, the processor job performs windowed aggregation. It uses standard Flink Tumbling Windows of 10 seconds and 5 minutes duration to build long-term history of event counts. Event counts are computed for multiple different groupings. First, event count aggregations are computed on a per-type basis. Further, they are also computed per-user and per-repository. Finally, in addition to the total event count, for repositories the job also compute only the count of new stars, which is used as an input to the trending score computation.

For real-time counters, standard sliding windows as provided by Flink proved too inefficient. Instead, the system uses a custom operator. This operator maintains circular buckets to aggregate counts efficiently across multiple overlapping window sizes of 5 minutes, 1 hour, 6 hours, and 24 hours simultaneously. This allows the dashboard to show the events in the last 24h updating every single second without the overhead of maintaining millions of window objects.

\paragraph{Ranking and Trending}
To generate the user and repository rankings, the process consumes the output of the real-time counters. A custom operator is used to compute and maintain the complete ranking for each category. However, outputting the complete ranking, or even just the rows that have shifted, for each time a live counter changes, would be prohibitively expensive. Instead, the operator simply outputs an update including only the row for which the count changed. However, crucially the updates produced by the operator include both the new and the old row number, allowing the frontend client to locally reconstruct exactly how the ranking changed.

For the global lists of trending repositories, the number of new stars is used. A custom operator calculates a composite score by combining new star counters across different time horizons, allowing the system to flag repositories experiencing a sudden surge in interest. The trending score is computed as
\begin{align*}
    t_\mathrm{score} = 10 s_{5\mathrm{m}} + 5 s_{5\mathrm{h}} + 2 s_{6\mathrm{h}} + s_{24\mathrm{h}} ,
\end{align*}
where $s_t$ is the number of new stars the repository has received in the latest time window of length $t$. This is quite a simple model, but works reasonably well to identify trending repositories. By assigning a higher weight to more recently obtained stars, repositories that are very popular in this very moment at valued more highly, but long term popularity is still taken into account.

\paragraph{Output of Results}
After performing the processing steps to compute the different analytics, the results are first written to PostgreSQL tables. Row updates are modeled as upserts, i.e., they represent an insert or an update of a single row. During this process each row is assigned a sequence number. This number is used by the client to know whether a row it receives from the frontend server is newer or older than the one it may already have. After writing the result to PostgreSQL, the row updates are sent to dedicated Kafka topics. Note that results are sent to Kafka only after they have been committed into PostgreSQL. This ordering ensures that if the frontend later starts listening to Kafka for real-time updates and then reads a snapshot from PostgreSQL, that there are no updates missing in the snapshot that already passed in the Kafka stream.

\subsubsection{Data Serving}
The processed data from Flink will be published to new Kafka topics. A Java-based server will consume these topics and expose the results via a WebSocket connection to the frontend.

\subsubsection{Frontend}
The user interface will be a custom single-page application using HTML and JavaScript, served by the frontend server. It will leverage the ECharts library to create an interactive dashboard with dynamic charts that update in real-time as new data arrives.

\subsubsection{Storage and Data Model}

\subsubsection{Messaging}


\subsection{Scalability and Reliability}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec nisi risus, suscipit sed mattis non, sagittis sed nisi. Sed a lobortis odio, quis dapibus nisl. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Fusce tincidunt sapien eu dolor ullamcorper, quis dapibus lectus venenatis. Nunc a blandit massa. Fusce ac fringilla nibh, nec eleifend mauris. Sed maximus tempus nibh, porta cursus mi rutrum et. Nunc erat massa, commodo id ornare ut, scelerisque ac est. Cras elit metus, aliquet sit amet maximus in, gravida in mauris. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Ut enim tellus, ornare quis malesuada non, scelerisque a elit. Donec dictum velit eu dui hendrerit ultrices. Pellentesque posuere porta odio eu feugiat. Phasellus non malesuada libero. Suspendisse sit amet tempus orci, quis dictum tellus. Proin magna massa. 
